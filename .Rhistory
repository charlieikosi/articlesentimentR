load_all()
library(devtools)
load_all()
check()
library(rvest)
install.packages("rvest")
install.packages("rvest")
library(rvest)
library(tidyverse)
library(dplyr)
library(tidytext)
library(tm)
article_sentiment <- function(url, lexicon_type){
URL <- URLencode(url)
page <- read_html(URL)
article <- page %>% html_nodes(".entry-content") %>%
html_text()
article <- as.data.frame(article)
article_tokens <- article %>%
unnest_tokens(word, article)
# Load and prepare the sentiment lexicon
sentiments <- get_sentiments(lexicon_type)
sentiments_df <- article_tokens %>%
inner_join(sentiments, by = "word")
sentiment_summary <- sentiments_df %>%
count(sentiment) %>%
mutate(proportion = n / sum(n))
# View the sentiment summary
sentiment_summary <- sentiment_summary[order(sentiment_summary$proportion, decreasing = TRUE),]
}
library(devtools)
load_all()
check()
check()
check()
check()
check()
check()
check()
roxygenise()
library(roxygen2)
roxygenise()
check()
roxygenise()
roxygenise()
check()
roxygenise()
check()
roxygenise()
?URLencoder()
?URLencode()
library(utils)
roxygenise()
library(rvest)
library(tidyverse)
library(dplyr)
library(tidytext)
library(tm)
library(utils)
article_sentiment <- function(url, lexicon_type){
URL <- URLencode(url)
page <- read_html(URL)
article <- page %>% html_nodes(".entry-content") %>%
html_text()
article <- as.data.frame(article)
article_tokens <- article %>%
unnest_tokens(word, article)
# Load and prepare the sentiment lexicon
sentiments <- get_sentiments(lexicon_type)
sentiments_df <- article_tokens %>%
inner_join(sentiments, by = "word")
sentiment_summary <- sentiments_df %>%
count(sentiment) %>%
mutate(proportion = n / sum(n))
# View the sentiment summary
sentiment_summary <- sentiment_summary[order(sentiment_summary$proportion, decreasing = TRUE),]
}
roxygenise()
rm(list = c("article_sentiment"))
roxygenise()
check()
roxygenise()
check()
roxygenise()
check()
document()
roxygenise()
roxygenise()
roxygenise()
document()
check()
